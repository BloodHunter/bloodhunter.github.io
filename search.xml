<?xml version="1.0" encoding="utf-8"?>
<search> 
  
    
    <entry>
      <title>Kafka源码分析——Consumer</title>
      <link href="/2019/01/13/kafka-yuan-ma-fen-xi-consumer/"/>
      <url>/2019/01/13/kafka-yuan-ma-fen-xi-consumer/</url>
      
        <content type="html"><![CDATA[<h2 id="Consumer使用实例"><a href="#Consumer使用实例" class="headerlink" title="Consumer使用实例"></a>Consumer使用实例</h2><h3 id="kafka-console-consumer"><a href="#kafka-console-consumer" class="headerlink" title="kafka-console-consumer"></a>kafka-console-consumer</h3><pre><code>sh kafka-console-consumer.sh ----bootstrap-server localhost:9092 --topic test --from-beginning</code></pre><h3 id="consumer-client"><a href="#consumer-client" class="headerlink" title="consumer client"></a>consumer client</h3><pre><code>Properties props = new Properties();props.put(&quot;bootstrap.servers&quot;,&quot;localhost:9092&quot;);    props.put(&quot;group.id&quot;,&quot;test_group_id&quot;);props.put(&quot;enable.auto.commit&quot;,&quot;true&quot;);props.put(&quot;auto.commit.interval.ms&quot;, &quot;1000&quot;);props.put(&quot;key.deserializer&quot;,&quot;org.apache.kafka.common.serialization.StringSerializer&quot;);props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;);KafkaConsumer&lt;String,String&gt; consumer = new KafkaConsumer&lt;&gt;(props);while(true){    ConsumerRecords&lt;String,String&gt; records = consumer.poll(1000)    for(ConsumerRecord&lt;String, String&gt; record : records){        System.out.printf(&quot;offset=%d,key=%s,value=%s&quot;,record.offset(),record.key(),record.value());    }}</code></pre><p>可以看到consumer的入口在poll方法，下面来看下poll方法的实现</p><h2 id="Consumer-poll模型"><a href="#Consumer-poll模型" class="headerlink" title="Consumer poll模型"></a>Consumer poll模型</h2><pre><code>//timeout是Consumer消费的超时时间，如果设置为0，表示buffer中只要有数据就立刻拉取public ConsumerRecords&lt;K, V&gt; poll(long timeout) {    acquire();    try {        if (timeout &lt; 0)            throw new IllegalArgumentException(&quot;Timeout must not be negative&quot;);        if (this.subscriptions.hasNoSubscriptionOrUserAssignment())            throw new IllegalStateException(&quot;Consumer is not subscribed to any topics or assigned any partitions&quot;);        // poll for new data until the timeout expires        long start = time.milliseconds();        long remaining = timeout;        do {                //从订阅的partition中消费数据，pollonce是其核心实现            Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = pollOnce(remaining);            if (!records.isEmpty()) {                // 在返回获取数据之前，需要发起下次的fetch请求，主要是为了避免用户在处理获取数据，而导致fetch请求被阻塞                if (fetcher.sendFetches() &gt; 0 || client.pendingRequestCount() &gt; 0)                    client.pollNoWakeup();                if (this.interceptors == null)                    return new ConsumerRecords&lt;&gt;(records);                else                    return this.interceptors.onConsume(new ConsumerRecords&lt;&gt;(records));            }            long elapsed = time.milliseconds() - start;            remaining = timeout - elapsed;        } while (remaining &gt; 0);        return ConsumerRecords.empty();    } finally {        release();    }}</code></pre><p>Consumer的poll方法主要在做以下几件事：</p><ol><li>检测timeout是否合法以及Consumer是否订阅了相应的topic-partition</li><li>调用pollOnce方法获取数据</li><li>在返回结果前，提前发起下次的fetch请求，避免用户在处理返回数据时，而导致线程被阻塞</li><li>如果在timeout的时间中没有获取到数据，则返回空数据</li></ol><h3 id="pollOnce方法"><a href="#pollOnce方法" class="headerlink" title="pollOnce方法"></a>pollOnce方法</h3><pre><code>private Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; pollOnce(long timeout) {    coordinator.poll(time.milliseconds());    // 确认是否所有的分区的offset是否有效，更新没有生效的partition的offset    if (!subscriptions.hasAllFetchPositions())        updateFetchPositions(this.subscriptions.missingFetchPositions());    // 如果获取到数据，则立马返回    Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; records = fetcher.fetchedRecords();    if (!records.isEmpty())        return records;    // 对于新的fetch请求，立即发起请求    fetcher.sendFetches();    long now = time.milliseconds();    long pollTimeout = Math.min(coordinator.timeToNextPoll(now), timeout);     //调用底层的poll方法，发起请求    client.poll(pollTimeout, now, new PollCondition() {        @Override        public boolean shouldBlock() {            // 对于已完成的fetch请求，则不进行阻塞            return !fetcher.hasCompletedFetches();        }    });    // 如果消费组group需要进行负责均衡rebalance，则直接返回空数据，    if (coordinator.needRejoin())        return Collections.emptyMap();    return fetcher.fetchedRecords();}    </code></pre><p>pollOnce方法，主要有以下几个步骤：</p><ol><li>coordinator.poll()</li><li>updateFetchPositions()</li><li>fetcher.fetchedRecords()</li><li>fetcher.sendFetches()</li><li>client.poll()</li><li>coordinator.needRejoin()</li></ol><p>下面详细分析以上几个步骤</p><h3 id="ConsumerCoordinator-poll"><a href="#ConsumerCoordinator-poll" class="headerlink" title="ConsumerCoordinator.poll()"></a>ConsumerCoordinator.poll()</h3><pre><code>//确保这个group的coordinator是已知的，并且已经Consumer已经加入到这个group中public void poll(long now) {    invokeCompletedOffsetCommitCallbacks();      //若订阅了topic，并且该coordinator是未知的，则初始化coordinator    if (subscriptions.partitionsAutoAssigned() &amp;&amp; coordinatorUnknown()) {        ensureCoordinatorReady();        now = time.milliseconds();    }     //Consumer是否需要重新加入到group中（如果partition发生变化，则需要rejoin）    if (needRejoin()) {        // due to a race condition between the initial metadata fetch and the initial rebalance,        // we need to ensure that the metadata is fresh before joining initially. This ensures        // that we have matched the pattern against the cluster&#39;s topics at least once before joining.        if (subscriptions.hasPatternSubscription())            client.ensureFreshMetadata();          // 确保group是active的        ensureActiveGroup();        now = time.milliseconds();    }     //检测心跳线程是否正常，若不正常，则抛出异常    pollHeartbeat(now);    //开启auto commit时，当定时时间到时则自动提交    maybeAutoCommitOffsetsAsync(now);}</code></pre><h3 id="updateFetchPositions"><a href="#updateFetchPositions" class="headerlink" title="updateFetchPositions()"></a>updateFetchPositions()</h3><pre><code>//如果有committed position，则将fetch position设置为committed position，否则使用配置的重置策略去设置offsetprivate void updateFetchPositions(Set&lt;TopicPartition&gt; partitions) {    //先重置那些需要重置的partition，比如调用了seekToBeginning，seekToEnd的partition    fetcher.resetOffsetsIfNeeded(partitions);    if (!subscriptions.hasAllFetchPositions(partitions)) {        // if we still don&#39;t have offsets for the given partitions, then we should either        // seek to the last committed position or reset using the auto reset policy        // first refresh commits for all assigned partitions        coordinator.refreshCommittedOffsetsIfNeeded();        // then do any offset lookups in case some positions are not known        fetcher.updateFetchPositions(partitions);    }}   </code></pre><h3 id="fetcher-fetchedRecords"><a href="#fetcher-fetchedRecords" class="headerlink" title="fetcher.fetchedRecords()"></a>fetcher.fetchedRecords()</h3><pre><code>public Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; fetchedRecords() {    if (nextInLineExceptionMetadata != null) {        ExceptionMetadata exceptionMetadata = nextInLineExceptionMetadata;        nextInLineExceptionMetadata = null;        TopicPartition tp = exceptionMetadata.partition;        if (subscriptions.isFetchable(tp) &amp;&amp; subscriptions.position(tp) == exceptionMetadata.fetchedOffset)            throw exceptionMetadata.exception;    }    Map&lt;TopicPartition, List&lt;ConsumerRecord&lt;K, V&gt;&gt;&gt; drained = new HashMap&lt;&gt;();    int recordsRemaining = maxPollRecords;    while (recordsRemaining &gt; 0) {        if (nextInLineRecords == null || nextInLineRecords.isDrained()) {            CompletedFetch completedFetch = completedFetches.poll();            if (completedFetch == null) break;            try {                nextInLineRecords = parseCompletedFetch(completedFetch);            } catch (KafkaException e) {                if (drained.isEmpty())                    throw e;                nextInLineExceptionMetadata = new ExceptionMetadata(completedFetch.partition, completedFetch.fetchedOffset, e);            }        } else {            TopicPartition partition = nextInLineRecords.partition;            List&lt;ConsumerRecord&lt;K, V&gt;&gt; records = drainRecords(nextInLineRecords, recordsRemaining);            if (!records.isEmpty()) {                List&lt;ConsumerRecord&lt;K, V&gt;&gt; currentRecords = drained.get(partition);                if (currentRecords == null) {                    drained.put(partition, records);                } else {                    // this case shouldn&#39;t usually happen because we only send one fetch at a time per partition,                    // but it might conceivably happen in some rare cases (such as partition leader changes).                    // we have to copy to a new list because the old one may be immutable                    List&lt;ConsumerRecord&lt;K, V&gt;&gt; newRecords = new ArrayList&lt;&gt;(records.size() + currentRecords.size());                    newRecords.addAll(currentRecords);                    newRecords.addAll(records);                    drained.put(partition, newRecords);                }                recordsRemaining -= records.size();            }        }    }    return drained;}</code></pre><h3 id="fetcher-sendFetches"><a href="#fetcher-sendFetches" class="headerlink" title="fetcher.sendFetches()"></a>fetcher.sendFetches()</h3><pre><code>//向订阅的所有的partition所在leader发送fetch请求public int sendFetches() {     //构建fetch请求    Map&lt;Node, FetchRequest.Builder&gt; fetchRequestMap = createFetchRequests();    for (Map.Entry&lt;Node, FetchRequest.Builder&gt; fetchEntry : fetchRequestMap.entrySet()) {        final FetchRequest.Builder request = fetchEntry.getValue();        final Node fetchTarget = fetchEntry.getKey();        log.debug(&quot;Sending fetch for partitions {} to broker {}&quot;, request.fetchData().keySet(), fetchTarget);        //发起fetch请求        client.send(fetchTarget, request)                .addListener(new RequestFutureListener&lt;ClientResponse&gt;() {                    @Override                    public void onSuccess(ClientResponse resp) {                        FetchResponse response = (FetchResponse) resp.responseBody();                        if (!matchesRequestedPartitions(request, response)) {                            // obviously we expect the broker to always send us valid responses, so this check                            // is mainly for test cases where mock fetch responses must be manually crafted.                            log.warn(&quot;Ignoring fetch response containing partitions {} since it does not match &quot; +                                    &quot;the requested partitions {}&quot;, response.responseData().keySet(),                                    request.fetchData().keySet());                            return;                        }                        Set&lt;TopicPartition&gt; partitions = new HashSet&lt;&gt;(response.responseData().keySet());                        FetchResponseMetricAggregator metricAggregator = new FetchResponseMetricAggregator(sensors, partitions);                        for (Map.Entry&lt;TopicPartition, FetchResponse.PartitionData&gt; entry : response.responseData().entrySet()) {                            TopicPartition partition = entry.getKey();                            long fetchOffset = request.fetchData().get(partition).offset;                            FetchResponse.PartitionData fetchData = entry.getValue();                            completedFetches.add(new CompletedFetch(partition, fetchOffset, fetchData, metricAggregator,                                    request.version()));                        }                        sensors.fetchLatency.record(resp.requestLatencyMs());                        sensors.fetchThrottleTimeSensor.record(response.getThrottleTime());                    }                    @Override                    public void onFailure(RuntimeException e) {                        log.debug(&quot;Fetch request to {} for partitions {} failed&quot;, fetchTarget, request.fetchData().keySet(), e);                    }                });    }    return fetchRequestMap.size();}    </code></pre><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li><a href="http://matt33.com/2017/11/11/consumer-pollonce/" target="_blank" rel="noopener">http://matt33.com/2017/11/11/consumer-pollonce/</a></li><li><a href="http://matt33.com/2017/10/22/consumer-join-group/" target="_blank" rel="noopener">http://matt33.com/2017/10/22/consumer-join-group/</a>    </li></ul>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kafka源码分析——Producer</title>
      <link href="/2019/01/05/kafka-yuan-ma-fen-xi-producer/"/>
      <url>/2019/01/05/kafka-yuan-ma-fen-xi-producer/</url>
      
        <content type="html"><![CDATA[<h2 id="Producer-使用示例"><a href="#Producer-使用示例" class="headerlink" title="Producer 使用示例"></a>Producer 使用示例</h2><h3 id="kafka-console-producer"><a href="#kafka-console-producer" class="headerlink" title="kafka-console-producer"></a>kafka-console-producer</h3><pre><code>sh kafka-console-producer.sh --broker-list localhost:9092 --topic test</code></pre><h3 id="producer-client"><a href="#producer-client" class="headerlink" title="producer client"></a>producer client</h3><pre><code>Properties props = new Properties(); props.put(&quot;bootstrap.servers&quot;, &quot;localhost:9092&quot;); props.put(&quot;acks&quot;, &quot;all&quot;); props.put(&quot;retries&quot;, 0); props.put(&quot;batch.size&quot;, 16384); props.put(&quot;linger.ms&quot;, 1); props.put(&quot;buffer.memory&quot;, 33554432); props.put(&quot;key.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); props.put(&quot;value.serializer&quot;, &quot;org.apache.kafka.common.serialization.StringSerializer&quot;); Producer&lt;String, String&gt; producer = new KafkaProducer&lt;&gt;(props); for(int i = 0; i &lt; 100; i++)     producer.send(new ProducerRecord&lt;String, String&gt;(&quot;my-topic&quot;, Integer.toString(i), Integer.toString(i))); producer.close();</code></pre><p>上述调用kafka Producer相关API，可以看到非常简单</p><pre><code>1. 生成Producer的配置，例如broker的地址，重试次数，key和value的序列化方式2. 调用KafkaProducer的send方法</code></pre><p>下面来看下Producer send的具体流程</p><h2 id="Producer-数据发送流程"><a href="#Producer-数据发送流程" class="headerlink" title="Producer 数据发送流程"></a>Producer 数据发送流程</h2><h3 id="KafkaProducer-send方法"><a href="#KafkaProducer-send方法" class="headerlink" title="KafkaProducer send方法"></a>KafkaProducer send方法</h3><pre><code>/** * * @param record 需要发送的数据    * @param callback 当数据发送成功调用的回调函数 *    */public Future&lt;RecordMetadata&gt; send(ProducerRecord&lt;K, V&gt; record, Callback callback) {    // intercept the record, which can be potentially modified; this method does not throw exceptions    ProducerRecord&lt;K, V&gt; interceptedRecord = this.interceptors == null ? record : this.interceptors.onSend(record);    return doSend(interceptedRecord, callback);}    </code></pre><p>可以看到真正的发送逻辑是在doSend方法中</p><h3 id="KafkaProducer-doSend方法"><a href="#KafkaProducer-doSend方法" class="headerlink" title="KafkaProducer doSend方法"></a>KafkaProducer doSend方法</h3><pre><code>private Future&lt;RecordMetadata&gt; doSend(ProducerRecord&lt;K, V&gt; record, Callback callback) {    TopicPartition tp = null;    try {        // 1. 检测topic的元数据是否可用        ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);        long remainingWaitMs = Math.max(0, maxBlockTimeMs - clusterAndWaitTime.waitedOnMetadataMs);        Cluster cluster = clusterAndWaitTime.cluster;        // 2. 对record的key和value进行序列化        byte[] serializedKey;        try {            serializedKey = keySerializer.serialize(record.topic(), record.key());        } catch (ClassCastException cce) {            throw new SerializationException(&quot;Can&#39;t convert key of class &quot; + record.key().getClass().getName() +                    &quot; to class &quot; + producerConfig.getClass(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG).getName() +                    &quot; specified in key.serializer&quot;);        }        byte[] serializedValue;        try {            serializedValue = valueSerializer.serialize(record.topic(), record.value());        } catch (ClassCastException cce) {            throw new SerializationException(&quot;Can&#39;t convert value of class &quot; + record.value().getClass().getName() +                    &quot; to class &quot; + producerConfig.getClass(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG).getName() +                    &quot; specified in value.serializer&quot;);        }          // 3. 确定需要发送的partition        int partition = partition(record, serializedKey, serializedValue, cluster);        int serializedSize = Records.LOG_OVERHEAD + Record.recordSize(serializedKey, serializedValue);        ensureValidRecordSize(serializedSize);        tp = new TopicPartition(record.topic(), partition);        long timestamp = record.timestamp() == null ? time.milliseconds() : record.timestamp();        log.trace(&quot;Sending record {} with callback {} to topic {} partition {}&quot;, record, callback, record.topic(), partition);        // producer callback will make sure to call both &#39;callback&#39; and interceptor callback        Callback interceptCallback = this.interceptors == null ? callback : new InterceptorCallback&lt;&gt;(callback, this.interceptors, tp);        // 4. 往RecordAccumulator追加record        RecordAccumulator.RecordAppendResult result = accumulator.append(tp, timestamp, serializedKey, serializedValue, interceptCallback, remainingWaitMs);        // 5. 如果batch已经满了，或者新的batch已经创建了，则唤醒send线程发送数据        if (result.batchIsFull || result.newBatchCreated) {            log.trace(&quot;Waking up the sender since topic {} partition {} is either full or getting a new batch&quot;, record.topic(), partition);            this.sender.wakeup();        }        return result.future;        // handling exceptions and record the errors;        // for API exceptions return them in the future,        // for other exceptions throw directly    } catch (ApiException e) {        log.debug(&quot;Exception occurred during message send:&quot;, e);        if (callback != null)            callback.onCompletion(null, e);        this.errors.record();        if (this.interceptors != null)            this.interceptors.onSendError(record, tp, e);        return new FutureFailure(e);    } catch (InterruptedException e) {        this.errors.record();        if (this.interceptors != null)            this.interceptors.onSendError(record, tp, e);        throw new InterruptException(e);    } catch (BufferExhaustedException e) {        this.errors.record();        this.metrics.sensor(&quot;buffer-exhausted-records&quot;).record();        if (this.interceptors != null)            this.interceptors.onSendError(record, tp, e);        throw e;    } catch (KafkaException e) {        this.errors.record();        if (this.interceptors != null)            this.interceptors.onSendError(record, tp, e);        throw e;    } catch (Exception e) {        // we notify interceptor about all exceptions, since onSend is called before anything else in this method        if (this.interceptors != null)            this.interceptors.onSendError(record, tp, e);        throw e;    }}</code></pre><p>doSend方法主要做了一下几件事情</p><h4 id="1-确认topic的元数据是否可用"><a href="#1-确认topic的元数据是否可用" class="headerlink" title="1. 确认topic的元数据是否可用"></a>1. 确认topic的元数据是否可用</h4><pre><code>ClusterAndWaitTime clusterAndWaitTime = waitOnMetadata(record.topic(), record.partition(), maxBlockTimeMs);</code></pre><h4 id="2-对record的key和value进行序列化"><a href="#2-对record的key和value进行序列化" class="headerlink" title="2. 对record的key和value进行序列化"></a>2. 对record的key和value进行序列化</h4><p>kafka提供了需要的序列化的方法，用户也可以根据需要自定义序列化方法，只要实现Serializer接口即可<br><img src="https://ws4.sinaimg.cn/large/006tNc79gy1fyvqrc8es2j31zq0kwais.jpg" alt=""></p><h4 id="3-确定需要发送的partition"><a href="#3-确定需要发送的partition" class="headerlink" title="3. 确定需要发送的partition"></a>3. 确定需要发送的partition</h4><pre><code>int partition = partition(record, serializedKey, serializedValue, cluster);private int partition(ProducerRecord&lt;K, V&gt; record, byte[] serializedKey, byte[] serializedValue, Cluster cluster) {    Integer partition = record.partition();    // 若指定了partition则使用指定的partition，若没指定则使用默认(DefaultPartitioner)的生成规则    return partition != null ?            partition :            partitioner.partition(                    record.topic(), record.key(), serializedKey, record.value(), serializedValue, cluster);}//DefaultPartitioner 中的partition方法public int partition(String topic, Object key, byte[] keyBytes, Object value, byte[] valueBytes, Cluster cluster) {    List&lt;PartitionInfo&gt; partitions = cluster.partitionsForTopic(topic);    int numPartitions = partitions.size();    // 若key为空，则随机生成一个num，利用num对partition的格式取余，同时保存num，下次在取num时，则对num递增即可    if (keyBytes == null) {        int nextValue = nextValue(topic);        List&lt;PartitionInfo&gt; availablePartitions = cluster.availablePartitionsForTopic(topic);        if (availablePartitions.size() &gt; 0) {            int part = Utils.toPositive(nextValue) % availablePartitions.size();            return availablePartitions.get(part).partition();        } else {            // no partitions are available, give a non-available partition            return Utils.toPositive(nextValue) % numPartitions;        }    } else {        // 若key不为空，则对key进行hash，并用得到的hash值对partition的个数进行取余        return Utils.toPositive(Utils.murmur2(keyBytes)) % numPartitions;    }}private int nextValue(String topic) {    AtomicInteger counter = topicCounterMap.get(topic);    // 若num为null，则随机取个num    if (null == counter) {        counter = new AtomicInteger(new Random().nextInt());        AtomicInteger currentCounter = topicCounterMap.putIfAbsent(topic, counter);        if (currentCounter != null) {            counter = currentCounter;        }    }    // 对num进行自增操作    return counter.getAndIncrement();}</code></pre><p>获取partition的流程图<br><img src="https://ws1.sinaimg.cn/large/006tNc79gy1fyvr5e6f0ij319i0nwn59.jpg" alt=""></p><h4 id="4-往RecordAccumulator追加record"><a href="#4-往RecordAccumulator追加record" class="headerlink" title="4. 往RecordAccumulator追加record"></a>4. 往RecordAccumulator追加record</h4><p>RecordAccumulator最重要的数据结构是batches，这是一个map，其中key是topicPartition，value是一个recordBatch的先进后出的队列, batchs的结构如下图所示。batchs每次从队尾append数据，从队头开始send数据</p><pre><code>private final ConcurrentMap&lt;TopicPartition, Deque&lt;RecordBatch&gt;&gt; batches;</code></pre><p><img src="https://ws3.sinaimg.cn/large/006tNc79gy1fyvrv76ah2j30py0f4juz.jpg" alt=""></p><pre><code>public RecordAppendResult append(TopicPartition tp,                                 long timestamp,                                 byte[] key,                                 byte[] value,                                 Callback callback,                                 long maxTimeToBlock) throws InterruptedException {    // abortIncompleteBatches().    appendsInProgress.incrementAndGet();    try {        // 获取对应topicPartition的Deque        Deque&lt;RecordBatch&gt; dq = getOrCreateDeque(tp);        //对deque进行append操作，会保证线程安全        synchronized (dq) {            if (closed)                throw new IllegalStateException(&quot;Cannot send after the producer is closed.&quot;);            // 开始追加数据            RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq);            // 当前队尾的recordBatch数据追加成功，无需新建recordBatch            if (appendResult != null)                return appendResult;        }        // 当前队尾无recordBatch，或者数据已满，需要新分配recordBatch        int size = Math.max(this.batchSize, Records.LOG_OVERHEAD + Record.recordSize(key, value));        log.trace(&quot;Allocating a new {} byte message buffer for topic {} partition {}&quot;, size, tp.topic(), tp.partition());        ByteBuffer buffer = free.allocate(size, maxTimeToBlock);        synchronized (dq) {            // Need to check if producer is closed again after grabbing the dequeue lock.            if (closed)                throw new IllegalStateException(&quot;Cannot send after the producer is closed.&quot;);            RecordAppendResult appendResult = tryAppend(timestamp, key, value, callback, dq);            // recordBatch已经创建，需要释放刚刚分配的buffer            if (appendResult != null) {                free.deallocate(buffer);                return appendResult;            }            MemoryRecordsBuilder recordsBuilder = MemoryRecords.builder(buffer, compression, TimestampType.CREATE_TIME, this.batchSize);            RecordBatch batch = new RecordBatch(tp, recordsBuilder, time.milliseconds());            FutureRecordMetadata future = Utils.notNull(batch.tryAppend(timestamp, key, value, callback, time.milliseconds()));                // 在deque中追加新建的recordBatch            dq.addLast(batch);            // 往未返回ack的队列中，增加刚刚创建的recordBatch            incomplete.add(batch);            // 如果队列中有多个recordBatch，那么最先创建的recordBatch，肯定是可以发送的，或者新建的recordBatch已满，则可以发送数据            return new RecordAppendResult(future, dq.size() &gt; 1 || batch.isFull(), true);        }    } finally {        appendsInProgress.decrementAndGet();    }}</code></pre><h4 id="5-唤醒send线程发送数据"><a href="#5-唤醒send线程发送数据" class="headerlink" title="5. 唤醒send线程发送数据"></a>5. 唤醒send线程发送数据</h4><p>如果发现recordBatch达到发送的要求，则唤醒send线程开始发送数据，下面来看下send线程中的run方法</p><pre><code>package org.apache.kafka.clients.producer.internals;void run(long now) {    Cluster cluster = metadata.fetch();    // 获取可以发送的recordBatch    RecordAccumulator.ReadyCheckResult result = this.accumulator.ready(cluster, now);    // 如果topicPartition的leader是未知，则强制更新metadata    if (!result.unknownLeaderTopics.isEmpty()) {        // The set of topics with unknown leader contains topics with leader election pending as well as        // topics which may have expired. Add the topic again to metadata to ensure it is included        // and request metadata update, since there are messages to send to the topic.        for (String topic : result.unknownLeaderTopics)            this.metadata.add(topic);        this.metadata.requestUpdate();    }    // 删除没有ready的node    Iterator&lt;Node&gt; iter = result.readyNodes.iterator();    long notReadyTimeout = Long.MAX_VALUE;    while (iter.hasNext()) {        Node node = iter.next();        if (!this.client.ready(node, now)) {            iter.remove();            notReadyTimeout = Math.min(notReadyTimeout, this.client.connectionDelay(node, now));        }    }    // 获取对应node可发送的RecordBatch，key为node id    Map&lt;Integer, List&lt;RecordBatch&gt;&gt; batches = this.accumulator.drain(cluster,                                                                     result.readyNodes,                                                                     this.maxRequestSize,                                                                     now);    if (guaranteeMessageOrder) {        // Mute all the partitions drained        for (List&lt;RecordBatch&gt; batchList : batches.values()) {            for (RecordBatch batch : batchList)                this.accumulator.mutePartition(batch.topicPartition);        }    }    // 删除超时的RecordBatch    List&lt;RecordBatch&gt; expiredBatches = this.accumulator.abortExpiredBatches(this.requestTimeout, now);    // update sensors    for (RecordBatch expiredBatch : expiredBatches)        this.sensors.recordErrors(expiredBatch.topicPartition.topic(), expiredBatch.recordCount);    sensors.updateProduceRequestMetrics(batches);    // If we have any nodes that are ready to send + have sendable data, poll with 0 timeout so this can immediately    // loop and try sending more data. Otherwise, the timeout is determined by nodes that have partitions with data    // that isn&#39;t yet sendable (e.g. lingering, backing off). Note that this specifically does not include nodes    // with sendable data that aren&#39;t ready to send since they would cause busy looping.    long pollTimeout = Math.min(result.nextReadyCheckDelayMs, notReadyTimeout);    if (!result.readyNodes.isEmpty()) {        log.trace(&quot;Nodes with data ready to send: {}&quot;, result.readyNodes);        pollTimeout = 0;    }    // 发送RecordBatch    sendProduceRequests(batches, now);    // if some partitions are already ready to be sent, the select time would be 0;    // otherwise if some partition already has some data accumulated but not ready yet,    // the select time will be the time difference between now and its linger expiry time;    // otherwise the select time will be the time difference between now and the metadata expiry time;    this.client.poll(pollTimeout, now);}</code></pre><p>可以看到具体的发送逻辑在sendProduceRequests</p><pre><code>private void sendProduceRequest(long now, int destination, short acks, int timeout, List&lt;RecordBatch&gt; batches) {    Map&lt;TopicPartition, MemoryRecords&gt; produceRecordsByPartition = new HashMap&lt;&gt;(batches.size());    final Map&lt;TopicPartition, RecordBatch&gt; recordsByPartition = new HashMap&lt;&gt;(batches.size());    // 将同一个topicPartition的RecordBatch放在一起发送    for (RecordBatch batch : batches) {        TopicPartition tp = batch.topicPartition;        produceRecordsByPartition.put(tp, batch.records());        recordsByPartition.put(tp, batch);    }    ProduceRequest.Builder requestBuilder =            new ProduceRequest.Builder(acks, timeout, produceRecordsByPartition);    RequestCompletionHandler callback = new RequestCompletionHandler() {        public void onComplete(ClientResponse response) {            handleProduceResponse(response, recordsByPartition, time.milliseconds());        }    };    String nodeId = Integer.toString(destination);    ClientRequest clientRequest = client.newClientRequest(nodeId, requestBuilder, now, acks != 0, callback);    client.send(clientRequest, now);    log.trace(&quot;Sent produce request to {}: {}&quot;, nodeId, requestBuilder);}</code></pre><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>总结一下Producer的发送流程</p><ol><li>确认topic的元数据是否可用</li><li>对key和value进行序列化</li><li>确定发送的partition</li><li>往RecordAccumulator追加record</li><li>如果满足发送条件，则唤醒sender线程发送数据</li></ol>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kafka——QuickStart(2)</title>
      <link href="/2018/12/20/kafka-quickstart-2/"/>
      <url>/2018/12/20/kafka-quickstart-2/</url>
      
        <content type="html"><![CDATA[<h2 id="安装"><a href="#安装" class="headerlink" title="安装"></a>安装</h2><h3 id="下载源码安装"><a href="#下载源码安装" class="headerlink" title="下载源码安装"></a>下载源码安装</h3><p><a href="https://kafka.apache.org/downloads" target="_blank" rel="noopener">下载地址</a></p><p>我用的kafka的版本是0.10.2.0，后续的例子都是使用这个版本</p><pre><code>tar -zxvf kafka-0.10.2.0-src.tgzcd kafka-0.10.2.0</code></pre><h3 id="Mac-Homebrew安装"><a href="#Mac-Homebrew安装" class="headerlink" title="Mac Homebrew安装"></a>Mac Homebrew安装</h3><pre><code>brew install kafka</code></pre><p>kafka的启动需要依赖zookeeper，用homebrew安装时，会自动安装zookeeper。安装完成之后，可以用以下命令查看安装信息</p><pre><code>brew info kafka</code></pre><p>kafka的安装路径，可以用以下命令查看</p><pre><code>brew list kakfa</code></pre><p>一般情况下，brew安装的项目路径为</p><pre><code>/usr/local/Cellar</code></pre><h2 id="启动"><a href="#启动" class="headerlink" title="启动"></a>启动</h2><h3 id="1-启动zookeeper"><a href="#1-启动zookeeper" class="headerlink" title="1. 启动zookeeper"></a>1. 启动zookeeper</h3><pre><code>cd /usr/local/Cellar/kafka/0.10.2.0/libexec/binsh zookeeper-server-start.sh ../config/zookeeper.properties</code></pre><p><img src="https://ws4.sinaimg.cn/large/006tNbRwgy1fyfqs9ohd9j327o0patom.jpg" alt=""></p><h3 id="2-启动Kafka-Server"><a href="#2-启动Kafka-Server" class="headerlink" title="2. 启动Kafka Server"></a>2. 启动Kafka Server</h3><pre><code>sh kafka-server-start.sh ../config/server.properties</code></pre><h3 id="3-创建Topic"><a href="#3-创建Topic" class="headerlink" title="3. 创建Topic"></a>3. 创建Topic</h3><pre><code>sh kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test</code></pre><h3 id="4-启动Producer"><a href="#4-启动Producer" class="headerlink" title="4. 启动Producer"></a>4. 启动Producer</h3><pre><code>sh kafka-console-producer.sh --broker-list localhost:9092 --topic test</code></pre><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fyfqzagbuvj31me07cwfr.jpg" alt="">    </p><h3 id="5-启动Consumer"><a href="#5-启动Consumer" class="headerlink" title="5. 启动Consumer"></a>5. 启动Consumer</h3><pre><code>sh kafka-console-consumer.sh --zookeeper localhost:2181 --topic test --from-beginning</code></pre><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fyfr1m23mqj327o08s0vr.jpg" alt=""></p><p>对于新的kafka版本，可以使用如下的命令</p><pre><code>sh kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning</code></pre><h2 id="kafka配置"><a href="#kafka配置" class="headerlink" title="kafka配置"></a>kafka配置</h2><h3 id="broker-配置"><a href="#broker-配置" class="headerlink" title="broker 配置"></a>broker 配置</h3><h4 id="基本配置"><a href="#基本配置" class="headerlink" title="基本配置"></a>基本配置</h4><table><thead><tr><th>配置名称</th><th>配置说明</th></tr></thead><tbody><tr><td>broker.id</td><td>broker在集群中的唯一标识</td></tr><tr><td>listeners</td><td>kafka监听的地址，如果没有配置，则使用java.net.InetAddress.getCanonicalHostName()获取的值</td></tr><tr><td>num.network.threads</td><td>处理网络请求的线程数</td></tr><tr><td>num.io.threads</td><td>处理I/O的线程数</td></tr><tr><td>socket.send.buffer.bytes</td><td>发送缓存区的大小</td></tr><tr><td>socket.receive.buffer.bytes</td><td>接收缓冲区的大小</td></tr><tr><td>socket.request.max.bytes</td><td>kafka允许接收或发送消息的最大字节数</td></tr></tbody></table><h4 id="zookeeper-配置"><a href="#zookeeper-配置" class="headerlink" title="zookeeper 配置"></a>zookeeper 配置</h4><table><thead><tr><th>配置名称</th><th>配置说明</th></tr></thead><tbody><tr><td>zookeeper.connect</td><td>zookeeper的连接地址，多个Server间以逗号分隔</td></tr><tr><td>zookeeper.connection.timeout.ms</td><td>连接zookeeper的超时时间</td></tr></tbody></table><h4 id="日志刷新策略"><a href="#日志刷新策略" class="headerlink" title="日志刷新策略"></a>日志刷新策略</h4><table><thead><tr><th>配置名称</th><th>配置说明</th></tr></thead><tbody><tr><td>log.flush.interval.messages</td><td>每次刷新至磁盘的消息数</td></tr><tr><td>log.flush.interval.ms</td><td>在数据被写入到硬盘前的最大时间</td></tr></tbody></table><h4 id="日志持久化策略"><a href="#日志持久化策略" class="headerlink" title="日志持久化策略"></a>日志持久化策略</h4><table><thead><tr><th>配置名称</th><th>配置说明</th></tr></thead><tbody><tr><td>log.retention.hours</td><td>日志保留的最长时间</td></tr><tr><td>log.retention.bytes</td><td>日志最大字节数</td></tr><tr><td>log.segment.bytes</td><td>单个log segment文件的大小</td></tr><tr><td>log.retention.check.interval.ms</td><td>检查log失效的间隔</td></tr></tbody></table><h3 id="producer-配置"><a href="#producer-配置" class="headerlink" title="producer 配置"></a>producer 配置</h3><table><thead><tr><th>配置名称</th><th>配置说明</th></tr></thead><tbody><tr><td>bootstrap.servers</td><td>broker地址</td></tr><tr><td>compression.type</td><td>数据压缩策略，none,gzip,snappy,lz4</td></tr><tr><td>partitioner.class</td><td>处理分区的类，默认根据key的hash分发到对应的分区</td></tr><tr><td>request.timeout.ms</td><td>请求的超时时间</td></tr></tbody></table><h3 id="consumer-配置"><a href="#consumer-配置" class="headerlink" title="consumer 配置"></a>consumer 配置</h3><table><thead><tr><th>配置名称</th><th>配置说明</th></tr></thead><tbody><tr><td>zookeeper.connect</td><td>zookeeper连接地址</td></tr><tr><td>zookeeper.connection.timeout.ms</td><td>zookeeper连接超时时间</td></tr><tr><td>group.id</td><td>消费组id</td></tr><tr><td>consumer.timeout.ms</td><td>消费者超时时间</td></tr></tbody></table><h2 id="kafka脚本参数说明"><a href="#kafka脚本参数说明" class="headerlink" title="kafka脚本参数说明"></a>kafka脚本参数说明</h2><h3 id="kafka-config"><a href="#kafka-config" class="headerlink" title="kafka-config"></a>kafka-config</h3><p>用于查看并修改kafka的配置，–describe 查看配置， –alter 修改配置</p><table><thead><tr><th>参数名称</th><th>参数说明</th></tr></thead><tbody><tr><td>entity-type</td><td>配置类型，有topics/clients/users/brokers</td></tr><tr><td>entiey-name</td><td>配置名称，对于topics就是topic的名称</td></tr></tbody></table><p>可以通过以下命令查看可管理的配置</p><pre><code>sh kafka-config.sh --help</code></pre><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fyfuab7stej30u016pdod.jpg" alt="">    </p><h4 id="describe"><a href="#describe" class="headerlink" title="describe"></a>describe</h4><pre><code>sh kafka-configs.sh --zookeeper localhost:2181 --describe --entity-type topics</code></pre><p><img src="https://ws3.sinaimg.cn/large/006tNbRwgy1fyftxq5ao7j320s04g0uv.jpg" alt=""></p><h4 id="alter"><a href="#alter" class="headerlink" title="alter"></a>alter</h4><pre><code>sh kafka-configs.sh --zookeeper localhost:2181 --alter --entity-type topics --entity-name test --add-config retention.ms=600000</code></pre><p>这个时候在看topic test的配置，发现配置已修改</p><p><img src="https://ws1.sinaimg.cn/large/006tNbRwgy1fyfu4kxpczj321o034wfw.jpg" alt="">    </p><h3 id="kafka-console-consumer"><a href="#kafka-console-consumer" class="headerlink" title="kafka-console-consumer"></a>kafka-console-consumer</h3><p>启动一个consumer</p><table><thead><tr><th>参数名称</th><th>参数说明</th></tr></thead><tbody><tr><td>bootstrap-server</td><td>broker地址，localhost:9092</td></tr><tr><td>zookeeper</td><td>zookeeper地址，localhost:2181</td></tr><tr><td>topic</td><td>topic名称</td></tr><tr><td>formatter</td><td>格式化消息的类的名称</td></tr><tr><td>from-beginning</td><td>如果consumer没有设置offset，则从最开始的消息开始消费，而不是最新的数据</td></tr><tr><td>offset</td><td>指定offset的位置，可以是正整数，也可以是earliest/latest，默认是latest</td></tr><tr><td>partition</td><td>指定从哪个partition开始消费数据</td></tr></tbody></table><h3 id="kafka-topics"><a href="#kafka-topics" class="headerlink" title="kafka-topics"></a>kafka-topics</h3><p>创建，删除，修改topic</p><table><thead><tr><th>参数名称</th><th>参数说明</th></tr></thead><tbody><tr><td>config</td><td>topic配置</td></tr><tr><td>delete-config</td><td>删除配置</td></tr><tr><td>create</td><td>创建topic</td></tr><tr><td>delete</td><td>删除topic</td></tr><tr><td>partitions</td><td>topic的分区数</td></tr><tr><td>replication-factor</td><td>topic备份的数</td></tr><tr><td>topic</td><td>topic名称</td></tr></tbody></table><h4 id="create-topic"><a href="#create-topic" class="headerlink" title="create topic"></a>create topic</h4><pre><code>sh kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 2 --topic test-topic</code></pre><h4 id="delete-topic"><a href="#delete-topic" class="headerlink" title="delete topic"></a>delete topic</h4><pre><code>sh kafka-topics.sh --delete -zookeeper localhost:2181 --topic test-topic</code></pre><h4 id="describe-topic"><a href="#describe-topic" class="headerlink" title="describe topic"></a>describe topic</h4><pre><code>sh kafka-topics.sh --zookeeper localhost:2181 --describe --topic test-topic</code></pre><h4 id="alter-topic"><a href="#alter-topic" class="headerlink" title="alter topic"></a>alter topic</h4><p>修改partitions和replica的个数，只能增加</p><pre><code>sh kafka-topics.sh --alter -zookeeper localhost:2181 --topic test-topic --partitions 3                </code></pre><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li><a href="https://kafka.apache.org/documentation/#configuration" target="_blank" rel="noopener">https://kafka.apache.org/documentation/#configuration</a></li><li><a href="https://my.oschina.net/u/1757002/blog/868517" target="_blank" rel="noopener">https://my.oschina.net/u/1757002/blog/868517</a></li><li><a href="https://www.jianshu.com/p/f94bb7a70ab6" target="_blank" rel="noopener">https://www.jianshu.com/p/f94bb7a70ab6</a></li><li><a href="https://www.jianshu.com/p/3ed342a28a9d" target="_blank" rel="noopener">https://www.jianshu.com/p/3ed342a28a9d</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
    <entry>
      <title>Kafka—-入门介绍(1)</title>
      <link href="/2018/12/16/kafka-ru-men-jie-shao-1/"/>
      <url>/2018/12/16/kafka-ru-men-jie-shao-1/</url>
      
        <content type="html"><![CDATA[<h2 id="Kafka介绍"><a href="#Kafka介绍" class="headerlink" title="Kafka介绍"></a>Kafka介绍</h2><p>kafka是一个分布式的，基于发布/订阅的消息系统。简单的可以理解kafka是一个消息队列，可以往队列里面写入数据，也可以从队列里面取出数据进行处理。</p><h2 id="kafka关键概念"><a href="#kafka关键概念" class="headerlink" title="kafka关键概念"></a>kafka关键概念</h2><p>我以自来水厂的例子来解释kafka的相关概念，可能不够严谨，只为方便大家理解。</p><p>从前有一家自来水厂(producer)负责把水运输到不同的地方，以供当地的居民(consumer)使用。冬天大家用水较少，但是水厂又一直在送水，导致水浪费了；到了夏天大家用水多，自来水来不及生产，导致居民无水可用。因此需要一个蓄水池(broker),自来水厂将水运输到蓄水池中，居民从蓄水池取水使用。蓄水池通过一个管道(topic)将水运输到不同的小区中</p><h3 id="Topic"><a href="#Topic" class="headerlink" title="Topic"></a>Topic</h3><p>topic可以理解数据标签，kafka通过topic对数据进行分门别类，就好比上述例子中的管道，使得自来水可以流向不同的地方，而不导致水混在一起。</p><h3 id="Producer"><a href="#Producer" class="headerlink" title="Producer"></a>Producer</h3><p>生产者，数据的来源，就好比上述例子中自来水厂，水都是从自来水来的。</p><h3 id="Consumer"><a href="#Consumer" class="headerlink" title="Consumer"></a>Consumer</h3><p>消费者，数据的处理者，就好比上述例子中的居民，居民需要取水喝。</p><h3 id="Broker"><a href="#Broker" class="headerlink" title="Broker"></a>Broker</h3><p>数据保存的地方，多个broker构成一个kafka集群。就好比上述例子中的蓄水池，生成者生成的数据都保存在broker中。</p><h2 id="Topic抽象"><a href="#Topic抽象" class="headerlink" title="Topic抽象"></a>Topic抽象</h2><p><img src="http://img.orchome.com:8888/group1/M00/00/01/KmCudlf7DsaAVF0WAABMe0J0lv4158.png" alt=""></p><p>topic是一个大的管道，但是为了提供吞吐量，在管道中有设置了许多小的通道(partition)，也就是分区。每一个分区都是一个<strong>顺序的</strong>，不可变的消息队列，并且可以持续添加。每个分区通过一个唯一的offset来标识消息处理的进度。</p><p>消费者可以控制offset，例如消费可以控制从最新的数据开始消费，即设置offset为new，也可以从最早的数据开始消费，即设置offset为early</p><h2 id="生成者"><a href="#生成者" class="headerlink" title="生成者"></a>生成者</h2><p>生产者负责往某个topic写入数据。由于topic有多个分区，数据可能会按照分区的顺序写入，也可以按照某种算法写入对应的分区，这个可以有开发者自己控制。</p><h2 id="消费者"><a href="#消费者" class="headerlink" title="消费者"></a>消费者</h2><p>消费者负责从topic中读取数据。kafka为消费者提供了一个抽象模型-消费组(consumer group)。消费组可以对应上述例子中小区，每一个居民都是消费者(consumer)，同一个小区的居民就是属于同一个消费组。</p><p>kafka之所以抽象消费组的概念，是为了兼容两种消费模型，队列模型和发布-订阅模型。对于队列来说，一组消费者从同一个服务器消费数据，一个消息只能由一个消费者消费。在发布-订阅模型中，一个消息被广播给所有的消费者。如果所有的消费者都在一个消费组中，则变成了队列模型；如果每一个消费者都在不同的消费组中，则变成了发布-订阅模型。</p><p><img src="http://img.orchome.com:8888/group1/M00/00/01/KmCudlf7D-OAEjy8AABoxGLnMI4173.png" alt=""></p><p>在kafka中，一个分区中的消息只能被同一个消费组中一个消费者消费。例如一个topic中有三个分区p1,p2,p3。消费组groupA，只有一个消费者A1；消费者groupB，有4个消费者，B1,B2,B3,B4。则消费情况可能如下所示，</p><p>对于消费组groupA，</p><table><thead><tr><th>partition</th><th>consumer</th></tr></thead><tbody><tr><td>p1</td><td>A1</td></tr><tr><td>p2</td><td>A1</td></tr><tr><td>p3</td><td>A1</td></tr></tbody></table><p>由于groupA只有一个consumer，所以所有的分区都由这个consumer消费</p><p>对于消费组groupB</p><table><thead><tr><th>partition</th><th>consumer</th></tr></thead><tbody><tr><td>p1</td><td>B1</td></tr><tr><td>p2</td><td>B2</td></tr><tr><td>p3</td><td>B3</td></tr></tbody></table><p>groupB有4个consumer，但是这个topic只有3个partition，所以有一个consumer将消费不到任何数据，除非其中一个consumer挂掉了，剩下空闲的这个consumer才会上位。</p><p><strong>一个partition的数据，只能由一个consumer group的一个consumer消费</strong></p><h2 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h2><ul><li><a href="https://kafka.apache.org/documentation/" target="_blank" rel="noopener">Apache Kafka</a></li><li><a href="https://www.infoq.cn/article/kafka-analysis-part-1" target="_blank" rel="noopener">Kafka 设计解析（一）：Kafka 背景及架构介绍</a></li><li><a href="http://orchome.com/5#/collapse-1005" target="_blank" rel="noopener">kafka入门介绍</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 后端 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Kafka </tag>
            
        </tags>
      
    </entry>
    
  
  
</search>
